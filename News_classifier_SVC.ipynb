{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# NEWS category detector\n",
    "\n",
    "This dataset contains around 200k news headlines from the year 2012 to 2018 obtained from HuffPost. The model trained on this dataset could be used to identify tags for untracked news articles or to identify the type of language used in different news articles.\n",
    "\n",
    "https://www.kaggle.com/rmisra/news-category-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import pickle as pk\n",
    "\n",
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display\n",
    "import time\n",
    "from time import time\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pre-processing\n",
    "### Importing data and creating a parsed out database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>headline</th>\n",
       "      <th>authors</th>\n",
       "      <th>link</th>\n",
       "      <th>short_description</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CRIME</td>\n",
       "      <td>There Were 2 Mass Shootings In Texas Last Week...</td>\n",
       "      <td>Melissa Jeltsen</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/texas-ama...</td>\n",
       "      <td>She left her husband. He killed their children...</td>\n",
       "      <td>2018-05-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Will Smith Joins Diplo And Nicky Jam For The 2...</td>\n",
       "      <td>Andy McDonald</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/will-smit...</td>\n",
       "      <td>Of course it has a song.</td>\n",
       "      <td>2018-05-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Hugh Grant Marries For The First Time At Age 57</td>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/hugh-gran...</td>\n",
       "      <td>The actor and his longtime girlfriend Anna Ebe...</td>\n",
       "      <td>2018-05-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Jim Carrey Blasts 'Castrato' Adam Schiff And D...</td>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/jim-carre...</td>\n",
       "      <td>The actor gives Dems an ass-kicking for not fi...</td>\n",
       "      <td>2018-05-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Julianna Margulies Uses Donald Trump Poop Bags...</td>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/julianna-...</td>\n",
       "      <td>The \"Dietland\" actress said using the bags is ...</td>\n",
       "      <td>2018-05-26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        category                                           headline  \\\n",
       "0          CRIME  There Were 2 Mass Shootings In Texas Last Week...   \n",
       "1  ENTERTAINMENT  Will Smith Joins Diplo And Nicky Jam For The 2...   \n",
       "2  ENTERTAINMENT    Hugh Grant Marries For The First Time At Age 57   \n",
       "3  ENTERTAINMENT  Jim Carrey Blasts 'Castrato' Adam Schiff And D...   \n",
       "4  ENTERTAINMENT  Julianna Margulies Uses Donald Trump Poop Bags...   \n",
       "\n",
       "           authors                                               link  \\\n",
       "0  Melissa Jeltsen  https://www.huffingtonpost.com/entry/texas-ama...   \n",
       "1    Andy McDonald  https://www.huffingtonpost.com/entry/will-smit...   \n",
       "2       Ron Dicker  https://www.huffingtonpost.com/entry/hugh-gran...   \n",
       "3       Ron Dicker  https://www.huffingtonpost.com/entry/jim-carre...   \n",
       "4       Ron Dicker  https://www.huffingtonpost.com/entry/julianna-...   \n",
       "\n",
       "                                   short_description       date  \n",
       "0  She left her husband. He killed their children... 2018-05-26  \n",
       "1                           Of course it has a song. 2018-05-26  \n",
       "2  The actor and his longtime girlfriend Anna Ebe... 2018-05-26  \n",
       "3  The actor gives Dems an ass-kicking for not fi... 2018-05-26  \n",
       "4  The \"Dietland\" actress said using the bags is ... 2018-05-26  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json('News_Category_Dataset_v2.json', lines=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category\n",
       "ARTS               1509\n",
       "ARTS & CULTURE     1339\n",
       "BLACK VOICES       4528\n",
       "BUSINESS           5937\n",
       "COLLEGE            1144\n",
       "COMEDY             5175\n",
       "CRIME              3405\n",
       "CULTURE & ARTS     1030\n",
       "DIVORCE            3426\n",
       "EDUCATION          1004\n",
       "ENTERTAINMENT     16058\n",
       "ENVIRONMENT        1323\n",
       "FIFTY              1401\n",
       "FOOD & DRINK       6226\n",
       "GOOD NEWS          1398\n",
       "GREEN              2622\n",
       "HEALTHY LIVING     6694\n",
       "HOME & LIVING      4195\n",
       "IMPACT             3459\n",
       "LATINO VOICES      1129\n",
       "MEDIA              2815\n",
       "MONEY              1707\n",
       "PARENTING          8677\n",
       "PARENTS            3955\n",
       "POLITICS          32739\n",
       "QUEER VOICES       6314\n",
       "RELIGION           2556\n",
       "SCIENCE            2178\n",
       "SPORTS             4884\n",
       "STYLE              2254\n",
       "STYLE & BEAUTY     9649\n",
       "TASTE              2096\n",
       "TECH               2082\n",
       "THE WORLDPOST      3664\n",
       "TRAVEL             9887\n",
       "WEDDINGS           3651\n",
       "WEIRD NEWS         2670\n",
       "WELLNESS          17827\n",
       "WOMEN              3490\n",
       "WORLD NEWS         2177\n",
       "WORLDPOST          2579\n",
       "dtype: int64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check size of each category\n",
    "df.groupby('category').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging categories\n",
    "df.category = df.category.map(lambda x: \"ARTS & CULTURE\" if x == \"ARTS\" else x)\n",
    "df.category = df.category.map(lambda x: \"ARTS & CULTURE\" if x == \"CULTURE & ARTS\" else x)\n",
    "df.category = df.category.map(lambda x: \"STYLE & BEAUTY\" if x == \"STYLE\" else x)\n",
    "df.category = df.category.map(lambda x: \"WORLDPOST\" if x == \"THE WORLDPOST\" else x)\n",
    "df.category = df.category.map(lambda x: \"ENVIRONMENT\" if x == \"GREEN\" else x)\n",
    "df.category = df.category.map(lambda x: \"PARENTS\" if x == \"PARENTING\" else x)\n",
    "df.category = df.category.map(lambda x: \"EDUCATION\" if x == \"COLLEGE\" else x)\n",
    "df.category = df.category.map(lambda x: \"HOME & LIVING\" if x == \"HEALTHY LIVING\" else x)\n",
    "df.category = df.category.map(lambda x: \"FOOD & DRINK\" if x == \"TASTE\" else x)\n",
    "df.category = df.category.map(lambda x: \"WORLD NEWS\" if x == \"WORLDPOST\" else x)\n",
    "\n",
    "# Remove categories to lower dataset size\n",
    "df = df[df['category'] != 'COMEDY']\n",
    "df = df[df['category'] != 'FIFTY']\n",
    "df = df[df['category'] != 'BLACK VOICES']\n",
    "df = df[df['category'] != 'IMPACT']\n",
    "df = df[df['category'] != 'GOOD NEWS']\n",
    "df = df[df['category'] != 'LATINO VOICES']\n",
    "df = df[df['category'] != 'PARENTS']\n",
    "df = df[df['category'] != 'QUEER VOICES']\n",
    "df = df[df['category'] != 'WEIRD NEWS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category\n",
       "ARTS & CULTURE     3878\n",
       "BUSINESS           5937\n",
       "CRIME              3405\n",
       "DIVORCE            3426\n",
       "EDUCATION          2148\n",
       "ENTERTAINMENT     16058\n",
       "ENVIRONMENT        3945\n",
       "FOOD & DRINK       8322\n",
       "HOME & LIVING     10889\n",
       "MEDIA              2815\n",
       "MONEY              1707\n",
       "POLITICS          32739\n",
       "RELIGION           2556\n",
       "SCIENCE            2178\n",
       "SPORTS             4884\n",
       "STYLE & BEAUTY    11903\n",
       "TECH               2082\n",
       "TRAVEL             9887\n",
       "WEDDINGS           3651\n",
       "WELLNESS          17827\n",
       "WOMEN              3490\n",
       "WORLD NEWS         8420\n",
       "dtype: int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Re-check size\n",
    "df.groupby('category').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(162147, 6)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge headline and description\n",
    "df['text'] = df['headline']+' '+df['short_description']\n",
    "df.drop(['short_description','headline'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>authors</th>\n",
       "      <th>link</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CRIME</td>\n",
       "      <td>Melissa Jeltsen</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/texas-ama...</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>There Were 2 Mass Shootings In Texas Last Week...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Andy McDonald</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/will-smit...</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>Will Smith Joins Diplo And Nicky Jam For The 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/hugh-gran...</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>Hugh Grant Marries For The First Time At Age 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/jim-carre...</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>Jim Carrey Blasts 'Castrato' Adam Schiff And D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/julianna-...</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>Julianna Margulies Uses Donald Trump Poop Bags...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200848</th>\n",
       "      <td>TECH</td>\n",
       "      <td>Reuters, Reuters</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/rim-ceo-t...</td>\n",
       "      <td>2012-01-28</td>\n",
       "      <td>RIM CEO Thorsten Heins' 'Significant' Plans Fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200849</th>\n",
       "      <td>SPORTS</td>\n",
       "      <td></td>\n",
       "      <td>https://www.huffingtonpost.com/entry/maria-sha...</td>\n",
       "      <td>2012-01-28</td>\n",
       "      <td>Maria Sharapova Stunned By Victoria Azarenka I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200850</th>\n",
       "      <td>SPORTS</td>\n",
       "      <td></td>\n",
       "      <td>https://www.huffingtonpost.com/entry/super-bow...</td>\n",
       "      <td>2012-01-28</td>\n",
       "      <td>Giants Over Patriots, Jets Over Colts Among  M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200851</th>\n",
       "      <td>SPORTS</td>\n",
       "      <td></td>\n",
       "      <td>https://www.huffingtonpost.com/entry/aldon-smi...</td>\n",
       "      <td>2012-01-28</td>\n",
       "      <td>Aldon Smith Arrested: 49ers Linebacker Busted ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200852</th>\n",
       "      <td>SPORTS</td>\n",
       "      <td></td>\n",
       "      <td>https://www.huffingtonpost.com/entry/dwight-ho...</td>\n",
       "      <td>2012-01-28</td>\n",
       "      <td>Dwight Howard Rips Teammates After Magic Loss ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>162147 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             category           authors  \\\n",
       "0               CRIME   Melissa Jeltsen   \n",
       "1       ENTERTAINMENT     Andy McDonald   \n",
       "2       ENTERTAINMENT        Ron Dicker   \n",
       "3       ENTERTAINMENT        Ron Dicker   \n",
       "4       ENTERTAINMENT        Ron Dicker   \n",
       "...               ...               ...   \n",
       "200848           TECH  Reuters, Reuters   \n",
       "200849         SPORTS                     \n",
       "200850         SPORTS                     \n",
       "200851         SPORTS                     \n",
       "200852         SPORTS                     \n",
       "\n",
       "                                                     link       date  \\\n",
       "0       https://www.huffingtonpost.com/entry/texas-ama... 2018-05-26   \n",
       "1       https://www.huffingtonpost.com/entry/will-smit... 2018-05-26   \n",
       "2       https://www.huffingtonpost.com/entry/hugh-gran... 2018-05-26   \n",
       "3       https://www.huffingtonpost.com/entry/jim-carre... 2018-05-26   \n",
       "4       https://www.huffingtonpost.com/entry/julianna-... 2018-05-26   \n",
       "...                                                   ...        ...   \n",
       "200848  https://www.huffingtonpost.com/entry/rim-ceo-t... 2012-01-28   \n",
       "200849  https://www.huffingtonpost.com/entry/maria-sha... 2012-01-28   \n",
       "200850  https://www.huffingtonpost.com/entry/super-bow... 2012-01-28   \n",
       "200851  https://www.huffingtonpost.com/entry/aldon-smi... 2012-01-28   \n",
       "200852  https://www.huffingtonpost.com/entry/dwight-ho... 2012-01-28   \n",
       "\n",
       "                                                     text  \n",
       "0       There Were 2 Mass Shootings In Texas Last Week...  \n",
       "1       Will Smith Joins Diplo And Nicky Jam For The 2...  \n",
       "2       Hugh Grant Marries For The First Time At Age 5...  \n",
       "3       Jim Carrey Blasts 'Castrato' Adam Schiff And D...  \n",
       "4       Julianna Margulies Uses Donald Trump Poop Bags...  \n",
       "...                                                   ...  \n",
       "200848  RIM CEO Thorsten Heins' 'Significant' Plans Fo...  \n",
       "200849  Maria Sharapova Stunned By Victoria Azarenka I...  \n",
       "200850  Giants Over Patriots, Jets Over Colts Among  M...  \n",
       "200851  Aldon Smith Arrested: 49ers Linebacker Busted ...  \n",
       "200852  Dwight Howard Rips Teammates After Magic Loss ...  \n",
       "\n",
       "[162147 rows x 5 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\laure\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Load english stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO BE COPIED IN DATA PREPARATION SCRIPT ###\n",
    "\n",
    "# Parsing text + cleaning + stemm\n",
    "def parse_out_text(all_text):\n",
    "    # clean punctuation, make lower case and remove stopwords\n",
    "    text_string = all_text.translate(str.maketrans(\"\", \"\", string.punctuation)).split(\" \")\n",
    "    text_string = [word.lower() for word in text_string if word.lower() not in stopwords.words('english')]\n",
    "    # Stemm text\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    stemmed = [stemmer.stem(word) for  word in text_string]\n",
    "    words = \" \".join(stemmed) \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Harvey Weinstein Accusers Say They Never Thought He Would Be Arrested Some of the actresses who came forward with sexual assault claims against the once powerful Hollywood mogul responded with both surprise and relief.'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text example\n",
    "text_test = df['text'].iloc[42]\n",
    "text_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'harvey weinstein accus say never thought would arrest actress came forward sexual assault claim power hollywood mogul respond surpris relief'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parsed text example\n",
    "parse_out_text(text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>category</th>\n",
       "      <th>authors</th>\n",
       "      <th>link</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>CRIME</td>\n",
       "      <td>Melissa Jeltsen</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/texas-ama...</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>There Were 2 Mass Shootings In Texas Last Week...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Andy McDonald</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/will-smit...</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>Will Smith Joins Diplo And Nicky Jam For The 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/hugh-gran...</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>Hugh Grant Marries For The First Time At Age 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/jim-carre...</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>Jim Carrey Blasts 'Castrato' Adam Schiff And D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/julianna-...</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>Julianna Margulies Uses Donald Trump Poop Bags...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index       category          authors  \\\n",
       "0      0          CRIME  Melissa Jeltsen   \n",
       "1      1  ENTERTAINMENT    Andy McDonald   \n",
       "2      2  ENTERTAINMENT       Ron Dicker   \n",
       "3      3  ENTERTAINMENT       Ron Dicker   \n",
       "4      4  ENTERTAINMENT       Ron Dicker   \n",
       "\n",
       "                                                link       date  \\\n",
       "0  https://www.huffingtonpost.com/entry/texas-ama... 2018-05-26   \n",
       "1  https://www.huffingtonpost.com/entry/will-smit... 2018-05-26   \n",
       "2  https://www.huffingtonpost.com/entry/hugh-gran... 2018-05-26   \n",
       "3  https://www.huffingtonpost.com/entry/jim-carre... 2018-05-26   \n",
       "4  https://www.huffingtonpost.com/entry/julianna-... 2018-05-26   \n",
       "\n",
       "                                                text  \n",
       "0  There Were 2 Mass Shootings In Texas Last Week...  \n",
       "1  Will Smith Joins Diplo And Nicky Jam For The 2...  \n",
       "2  Hugh Grant Marries For The First Time At Age 5...  \n",
       "3  Jim Carrey Blasts 'Castrato' Adam Schiff And D...  \n",
       "4  Julianna Margulies Uses Donald Trump Poop Bags...  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Resetting index (num rows = id)\n",
    "df = df.reset_index()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a column with parsed-out text and transform whole dataset\n",
    "\n",
    "def create_parsed_df(extract_df):\n",
    "\n",
    "    pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "    extract_df['parsed_text'] = 'NaN'\n",
    "    extract_df.drop(['index','authors','link','date'],axis=1, inplace=True)\n",
    "\n",
    "    max_count = extract_df.shape[0]\n",
    "\n",
    "    f = IntProgress(min=0, max=max_count) # instantiate the bar\n",
    "    display(f) # display the bar\n",
    "\n",
    "    for i in range(len(extract_df['text'])):\n",
    "        f.value += 1\n",
    "        item = parse_out_text(extract_df['text'].iloc[i])\n",
    "        extract_df['parsed_text'][i] = item\n",
    "    \n",
    "    extract_df = extract_df[['parsed_text','category']]\n",
    "    \n",
    "    # dump dataset in a pickle file \n",
    "    with open(\"df_parsed_text.pkl\", 'wb') as file:\n",
    "        pk.dump(extract_df, file)\n",
    "    \n",
    "    extract_df.to_csv('data.csv')\n",
    "    \n",
    "    return extract_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ede10e3438b4a3187ca0ab0448d336b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, max=162147)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parsed_text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2 mass shoot texa last week 1 tv left husband ...</td>\n",
       "      <td>CRIME</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>smith join diplo nicki jam 2018 world cup offi...</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hugh grant marri first time age 57 actor longt...</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jim carrey blast castrato adam schiff democrat...</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>julianna marguli use donald trump poop bag pic...</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162142</th>\n",
       "      <td>rim ceo thorsten hein signific plan blackberri...</td>\n",
       "      <td>TECH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162143</th>\n",
       "      <td>maria sharapova stun victoria azarenka austral...</td>\n",
       "      <td>SPORTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162144</th>\n",
       "      <td>giant patriot jet colt among  improb super bow...</td>\n",
       "      <td>SPORTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162145</th>\n",
       "      <td>aldon smith arrest 49er lineback bust dui corr...</td>\n",
       "      <td>SPORTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162146</th>\n",
       "      <td>dwight howard rip teammat magic loss hornet fi...</td>\n",
       "      <td>SPORTS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>162147 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              parsed_text       category\n",
       "0       2 mass shoot texa last week 1 tv left husband ...          CRIME\n",
       "1       smith join diplo nicki jam 2018 world cup offi...  ENTERTAINMENT\n",
       "2       hugh grant marri first time age 57 actor longt...  ENTERTAINMENT\n",
       "3       jim carrey blast castrato adam schiff democrat...  ENTERTAINMENT\n",
       "4       julianna marguli use donald trump poop bag pic...  ENTERTAINMENT\n",
       "...                                                   ...            ...\n",
       "162142  rim ceo thorsten hein signific plan blackberri...           TECH\n",
       "162143  maria sharapova stun victoria azarenka austral...         SPORTS\n",
       "162144  giant patriot jet colt among  improb super bow...         SPORTS\n",
       "162145  aldon smith arrest 49er lineback bust dui corr...         SPORTS\n",
       "162146  dwight howard rip teammat magic loss hornet fi...         SPORTS\n",
       "\n",
       "[162147 rows x 2 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## TAKES SOME TIME AND REPLACES df_parsed_text.pkl -> DON'T TOUCH WITHOUT GOOD REASON !\n",
    "create_parsed_df(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dumping an extract of the full db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parsed_text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2 mass shoot texa last week 1 tv left husband ...</td>\n",
       "      <td>CRIME</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>smith join diplo nicki jam 2018 world cup offi...</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hugh grant marri first time age 57 actor longt...</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jim carrey blast castrato adam schiff democrat...</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>julianna marguli use donald trump poop bag pic...</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162142</th>\n",
       "      <td>rim ceo thorsten hein signific plan blackberri...</td>\n",
       "      <td>TECH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162143</th>\n",
       "      <td>maria sharapova stun victoria azarenka austral...</td>\n",
       "      <td>SPORTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162144</th>\n",
       "      <td>giant patriot jet colt among  improb super bow...</td>\n",
       "      <td>SPORTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162145</th>\n",
       "      <td>aldon smith arrest 49er lineback bust dui corr...</td>\n",
       "      <td>SPORTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162146</th>\n",
       "      <td>dwight howard rip teammat magic loss hornet fi...</td>\n",
       "      <td>SPORTS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>162147 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              parsed_text       category\n",
       "0       2 mass shoot texa last week 1 tv left husband ...          CRIME\n",
       "1       smith join diplo nicki jam 2018 world cup offi...  ENTERTAINMENT\n",
       "2       hugh grant marri first time age 57 actor longt...  ENTERTAINMENT\n",
       "3       jim carrey blast castrato adam schiff democrat...  ENTERTAINMENT\n",
       "4       julianna marguli use donald trump poop bag pic...  ENTERTAINMENT\n",
       "...                                                   ...            ...\n",
       "162142  rim ceo thorsten hein signific plan blackberri...           TECH\n",
       "162143  maria sharapova stun victoria azarenka austral...         SPORTS\n",
       "162144  giant patriot jet colt among  improb super bow...         SPORTS\n",
       "162145  aldon smith arrest 49er lineback bust dui corr...         SPORTS\n",
       "162146  dwight howard rip teammat magic loss hornet fi...         SPORTS\n",
       "\n",
       "[162147 rows x 2 columns]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load full parsed dataset\n",
    "with open(\"df_parsed_text.pkl\", 'rb') as fid:\n",
    "    df_full = pk.load(fid)\n",
    "df_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>parsed_text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>155157</td>\n",
       "      <td>kathi griffin divorc comedian talk tattoo wed ...</td>\n",
       "      <td>DIVORCE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>120377</td>\n",
       "      <td>live world need help liter stuck foyer two ent...</td>\n",
       "      <td>DIVORCE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>101466</td>\n",
       "      <td>difficult part second marriag us betray past t...</td>\n",
       "      <td>DIVORCE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>118563</td>\n",
       "      <td>divorc women truth husband 401k asset marriag ...</td>\n",
       "      <td>DIVORCE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>157378</td>\n",
       "      <td>perfect divorc toolkit divorc toolkit use hamm...</td>\n",
       "      <td>DIVORCE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16211</th>\n",
       "      <td>75408</td>\n",
       "      <td>eight delici way day matzah shakshuka mani rec...</td>\n",
       "      <td>FOOD &amp; DRINK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16212</th>\n",
       "      <td>101956</td>\n",
       "      <td>sanitari cute way dri silverwar form ceram eleph</td>\n",
       "      <td>FOOD &amp; DRINK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16213</th>\n",
       "      <td>110899</td>\n",
       "      <td>eat tell peopl around photo gem need get away ...</td>\n",
       "      <td>FOOD &amp; DRINK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16214</th>\n",
       "      <td>149754</td>\n",
       "      <td>make cocktail without recip finish touch top c...</td>\n",
       "      <td>FOOD &amp; DRINK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16215</th>\n",
       "      <td>31068</td>\n",
       "      <td>one resolut could help save money eat better t...</td>\n",
       "      <td>FOOD &amp; DRINK</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16216 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        index                                        parsed_text      category\n",
       "0      155157  kathi griffin divorc comedian talk tattoo wed ...       DIVORCE\n",
       "1      120377  live world need help liter stuck foyer two ent...       DIVORCE\n",
       "2      101466  difficult part second marriag us betray past t...       DIVORCE\n",
       "3      118563  divorc women truth husband 401k asset marriag ...       DIVORCE\n",
       "4      157378  perfect divorc toolkit divorc toolkit use hamm...       DIVORCE\n",
       "...       ...                                                ...           ...\n",
       "16211   75408  eight delici way day matzah shakshuka mani rec...  FOOD & DRINK\n",
       "16212  101956   sanitari cute way dri silverwar form ceram eleph  FOOD & DRINK\n",
       "16213  110899  eat tell peopl around photo gem need get away ...  FOOD & DRINK\n",
       "16214  149754  make cocktail without recip finish touch top c...  FOOD & DRINK\n",
       "16215   31068  one resolut could help save money eat better t...  FOOD & DRINK\n",
       "\n",
       "[16216 rows x 3 columns]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reduce number of lines (123 GiB needed else...)\n",
    "def reduce_df(df, fraction):\n",
    "    df_reduced = pd.DataFrame(columns=['parsed_text','category'])\n",
    "    for cat in set(df_full['category']):\n",
    "        # Randomly sample x% of your dataframe\n",
    "        df_reduced = df_reduced.append(df[df['category'] == cat].sample(frac=fraction))\n",
    "    return df_reduced.reset_index()\n",
    "\n",
    "df_parsed = reduce_df(df_full,0.1)\n",
    "df_parsed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform data to numerical\n",
    "### Reduce dataset size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16216, 2)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_parsed.drop('index',axis=1, inplace=True)\n",
    "df_parsed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category\n",
       "ARTS & CULTURE     388\n",
       "BUSINESS           594\n",
       "CRIME              340\n",
       "DIVORCE            343\n",
       "EDUCATION          215\n",
       "ENTERTAINMENT     1606\n",
       "ENVIRONMENT        394\n",
       "FOOD & DRINK       832\n",
       "HOME & LIVING     1089\n",
       "MEDIA              282\n",
       "MONEY              171\n",
       "POLITICS          3274\n",
       "RELIGION           256\n",
       "SCIENCE            218\n",
       "SPORTS             488\n",
       "STYLE & BEAUTY    1190\n",
       "TECH               208\n",
       "TRAVEL             989\n",
       "WEDDINGS           365\n",
       "WELLNESS          1783\n",
       "WOMEN              349\n",
       "WORLD NEWS         842\n",
       "dtype: int64"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check number of points per category\n",
    "df_parsed.groupby('category').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsed_text    0\n",
      "category       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for NaN and drop them\n",
    "print(df_parsed.isna().sum())\n",
    "df_parsed = df_parsed.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check for most common words for a category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over all the words in all the texts and increment the counts in the appropriate counter objects\n",
    "def count_most_common(cat):\n",
    "    count_word = Counter()\n",
    "    df_cat = df_parsed[df_parsed['category'] == cat]\n",
    "    for article in df_cat['parsed_text']:\n",
    "        for word in article.split(\" \"):\n",
    "            count_word[word] += 1\n",
    "    return count_word.most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count_most_common('TECH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6min 7s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parsed_text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>divorc talk wed one like wed divorc</td>\n",
       "      <td>DIVORCE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>live need help two one</td>\n",
       "      <td>DIVORCE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>marriag us</td>\n",
       "      <td>DIVORCE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>divorc women husband marriag divorc time work ...</td>\n",
       "      <td>DIVORCE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>divorc divorc thing life divorc life</td>\n",
       "      <td>DIVORCE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16211</th>\n",
       "      <td>delici way day mani recip dish way make say be...</td>\n",
       "      <td>FOOD &amp; DRINK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16212</th>\n",
       "      <td>way</td>\n",
       "      <td>FOOD &amp; DRINK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16213</th>\n",
       "      <td>eat peopl photo need get</td>\n",
       "      <td>FOOD &amp; DRINK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16214</th>\n",
       "      <td>make cocktail recip top cocktail</td>\n",
       "      <td>FOOD &amp; DRINK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16215</th>\n",
       "      <td>one eat better one</td>\n",
       "      <td>FOOD &amp; DRINK</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16216 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             parsed_text      category\n",
       "0                    divorc talk wed one like wed divorc       DIVORCE\n",
       "1                                 live need help two one       DIVORCE\n",
       "2                                             marriag us       DIVORCE\n",
       "3      divorc women husband marriag divorc time work ...       DIVORCE\n",
       "4                   divorc divorc thing life divorc life       DIVORCE\n",
       "...                                                  ...           ...\n",
       "16211  delici way day mani recip dish way make say be...  FOOD & DRINK\n",
       "16212                                                way  FOOD & DRINK\n",
       "16213                           eat peopl photo need get  FOOD & DRINK\n",
       "16214                   make cocktail recip top cocktail  FOOD & DRINK\n",
       "16215                                 one eat better one  FOOD & DRINK\n",
       "\n",
       "[16216 rows x 2 columns]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "def keep_only_most_common(x):\n",
    "    word_count = count_most_common(x['category'])\n",
    "    word_list = [tpl[0] for tpl in word_count]\n",
    "    keep_word = [word for word in x['parsed_text'].split(\" \") if word in word_list]\n",
    "    return \" \".join(keep_word)\n",
    "\n",
    "df_parsed['parsed_text'] = df_parsed.apply(keep_only_most_common, axis=1)\n",
    "df_parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parsed_text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>divorc talk wed one like wed divorc</td>\n",
       "      <td>DIVORCE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>live need help two one</td>\n",
       "      <td>DIVORCE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>marriag us</td>\n",
       "      <td>DIVORCE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>divorc women husband marriag divorc time work ...</td>\n",
       "      <td>DIVORCE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>divorc divorc thing life divorc life</td>\n",
       "      <td>DIVORCE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         parsed_text category\n",
       "0                divorc talk wed one like wed divorc  DIVORCE\n",
       "1                             live need help two one  DIVORCE\n",
       "2                                         marriag us  DIVORCE\n",
       "3  divorc women husband marriag divorc time work ...  DIVORCE\n",
       "4               divorc divorc thing life divorc life  DIVORCE"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_parsed = df_parsed[df_parsed['parsed_text'] != '']\n",
    "df_parsed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ARTS & CULTURE',\n",
       " 'BUSINESS',\n",
       " 'CRIME',\n",
       " 'DIVORCE',\n",
       " 'EDUCATION',\n",
       " 'ENTERTAINMENT',\n",
       " 'ENVIRONMENT',\n",
       " 'FOOD & DRINK',\n",
       " 'HOME & LIVING',\n",
       " 'MEDIA',\n",
       " 'MONEY',\n",
       " 'POLITICS',\n",
       " 'RELIGION',\n",
       " 'SCIENCE',\n",
       " 'SPORTS',\n",
       " 'STYLE & BEAUTY',\n",
       " 'TECH',\n",
       " 'TRAVEL',\n",
       " 'WEDDINGS',\n",
       " 'WELLNESS',\n",
       " 'WOMEN',\n",
       " 'WORLD NEWS'}"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of features and labels\n",
    "parsed_text = list(df_parsed['parsed_text'])\n",
    "categories = list(df_parsed['category'])\n",
    "set(categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(parsed_text)\n",
    "X = X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['care',\n",
       " 'carpet',\n",
       " 'case',\n",
       " 'cast',\n",
       " 'catch',\n",
       " 'cathol',\n",
       " 'caus',\n",
       " 'celebr',\n",
       " 'ceo',\n",
       " 'challeng']"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = vectorizer.get_feature_names()\n",
    "words[100:110]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump classifier in a pickle file\n",
    "with open(\"vectorizer.pkl\", 'wb') as file:\n",
    "    pk.dump(vectorizer, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15552, 814)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  0, ..., 21, 21, 21])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert categrory words into integers\n",
    "word_to_int = dict((c, i) for i, c in enumerate(set(df_parsed['category'])))\n",
    "labels = [word_to_int[cat] for cat in categories]\n",
    "y = np.asarray(labels)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'DIVORCE',\n",
       " 1: 'CRIME',\n",
       " 2: 'MONEY',\n",
       " 3: 'HOME & LIVING',\n",
       " 4: 'WORLD NEWS',\n",
       " 5: 'WELLNESS',\n",
       " 6: 'RELIGION',\n",
       " 7: 'WEDDINGS',\n",
       " 8: 'TECH',\n",
       " 9: 'ENVIRONMENT',\n",
       " 10: 'ENTERTAINMENT',\n",
       " 11: 'SPORTS',\n",
       " 12: 'SCIENCE',\n",
       " 13: 'WOMEN',\n",
       " 14: 'EDUCATION',\n",
       " 15: 'TRAVEL',\n",
       " 16: 'POLITICS',\n",
       " 17: 'BUSINESS',\n",
       " 18: 'MEDIA',\n",
       " 19: 'STYLE & BEAUTY',\n",
       " 20: 'ARTS & CULTURE',\n",
       " 21: 'FOOD & DRINK'}"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_to_word = {v: k for k, v in word_to_int.items()}\n",
    "int_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump categories in a pickle file\n",
    "with open(\"categ.pkl\", 'wb') as file:\n",
    "    pk.dump((int_to_word), file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine learning\n",
    "### Genetrate train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump the sets to pkl files in a pickle file\n",
    "with open(\"X_train.pkl\", 'wb') as file:\n",
    "    pk.dump((X_train), file)\n",
    "    \n",
    "with open(\"X_test.pkl\", 'wb') as file:\n",
    "    pk.dump((X_test), file)\n",
    "\n",
    "with open(\"y_train.pkl\", 'wb') as file:\n",
    "    pk.dump((y_train), file)\n",
    "    \n",
    "with open(\"y_test.pkl\", 'wb') as file:\n",
    "    pk.dump((y_test), file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint for machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"X_train.pkl\", 'rb') as fid:\n",
    "    X_train = pk.load(fid)\n",
    "\n",
    "with open(\"X_test.pkl\", 'rb') as fid:\n",
    "    X_test = pk.load(fid)\n",
    "\n",
    "with open(\"y_train.pkl\", 'rb') as fid:\n",
    "    y_train = pk.load(fid)\n",
    "    \n",
    "with open(\"y_test.pkl\", 'rb') as fid:\n",
    "    y_test = pk.load(fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12441, 814)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12441,)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-validator : ShuffleSplit \n",
    "sss = StratifiedShuffleSplit(n_splits = 10, test_size = 0.2, random_state = 42) # To avoid over-fitting\n",
    "\n",
    "### Import classifier ###\n",
    "from sklearn.svm import LinearSVC\n",
    "clf = LinearSVC()\n",
    "\n",
    "# definition of the pipeline\n",
    "pipeline = Pipeline(steps = [\n",
    "    (\"LSVC\",clf)\n",
    "])\n",
    "\n",
    "# parameters to tune \n",
    "param_grid = {\n",
    "    'LSVC__C' : [1],\n",
    "    'LSVC__class_weight' : ['balanced'],\n",
    "    'LSVC__multi_class' : ['ovr'],\n",
    "    'LSVC__random_state' : [42],\n",
    "    'LSVC__max_iter' : [10000],\n",
    "}  \n",
    "\n",
    "# exhaustive search over specified parameter\n",
    "grid = GridSearchCV(pipeline, param_grid, verbose = 1, cv = sss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > training classifier:\n",
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:   31.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training time:  36.199 s\n"
     ]
    }
   ],
   "source": [
    "# training classifier\n",
    "print (\" > training classifier:\")\n",
    "t0 = time()\n",
    "grid.fit(X_train, y_train)\n",
    "print (\"training time: \", round(time()-t0, 3), \"s\")\n",
    "\n",
    "# best classifier using the cross-validator and the Stratified Shuffle Split \n",
    "clf = grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.84      0.87        68\n",
      "           1       0.89      0.85      0.87        66\n",
      "           2       0.63      0.91      0.75        34\n",
      "           3       0.67      0.66      0.66       205\n",
      "           4       0.79      0.80      0.79       157\n",
      "           5       0.83      0.79      0.81       352\n",
      "           6       0.76      0.78      0.77        49\n",
      "           7       0.96      0.89      0.92        73\n",
      "           8       0.80      0.80      0.80        41\n",
      "           9       0.84      0.68      0.75        76\n",
      "          10       0.70      0.82      0.76       290\n",
      "          11       0.85      0.77      0.81        92\n",
      "          12       0.52      0.64      0.57        42\n",
      "          13       0.66      0.72      0.69        67\n",
      "          14       0.69      0.88      0.77        41\n",
      "          15       0.92      0.83      0.88       193\n",
      "          16       0.91      0.88      0.89       628\n",
      "          17       0.69      0.73      0.71       115\n",
      "          18       0.52      0.61      0.56        54\n",
      "          19       0.87      0.87      0.87       233\n",
      "          20       0.90      0.71      0.79        73\n",
      "          21       0.89      0.88      0.88       162\n",
      "\n",
      "    accuracy                           0.81      3111\n",
      "   macro avg       0.78      0.79      0.78      3111\n",
      "weighted avg       0.82      0.81      0.81      3111\n",
      "\n",
      "Wall time: 53.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(classification_report(y_test, clf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump classifier in a pickle file\n",
    "with open(\"classifier.pkl\", 'wb') as file:\n",
    "    pk.dump(clf, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import pickle as pk\n",
    "\n",
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display\n",
    "import time\n",
    "from time import time\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle as pk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load vectorizer\n",
    "with open(\"vectorizer.pkl\", 'rb') as fid:\n",
    "    vectorizer_trained = pk.load(fid)\n",
    "\n",
    "# Load categories\n",
    "with open(\"categ.pkl\", 'rb') as fid:\n",
    "    categ = pk.load(fid)\n",
    "    \n",
    "# Load categories\n",
    "with open(\"classifier.pkl\", 'rb') as fid:\n",
    "    trained_clf = pk.load(fid)\n",
    "\n",
    "# Parsing text + cleaning + stemm\n",
    "def parse_out_text(all_text):\n",
    "    # clean punctuation, make lower case and remove stopwords\n",
    "    text_string = all_text.translate(str.maketrans(\"\", \"\", string.punctuation)).split(\" \")\n",
    "    text_string = [word.lower() for word in text_string if word.lower() not in stopwords.words('english')]\n",
    "    # Stemm text\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    stemmed = [stemmer.stem(word) for  word in text_string]\n",
    "    words = \" \".join(stemmed) \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_predict = 'If you think you might be ready to fly again, we can help plan your trip'\n",
    "text_to_predict = 'Google just created the most detailed image of a brain yet'\n",
    "text_to_predict = 'The mysterious skeleton of a woman who died more than 4,000 years ago has been discovered in Germany.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_text = parse_out_text(text_to_predict)\n",
    "feat_array = vectorizer_trained.transform([prepared_text]).toarray()\n",
    "result_int = trained_clf.predict(feat_array)[0]\n",
    "output = categ[result_int]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SCIENCE'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
